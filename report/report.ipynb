{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Assignment 1 Report\n",
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1-a\n",
    "\n",
    "![1nn](images/1nn.png)\n",
    "\n",
    "<center>\n",
    "$ 1nn(s_1) $ is <span style=\"color:red\">red</span> circle, <br>\n",
    "$ 1nn(s_2) $ is <span style=\"color:blue\">blue</span> circle, <br>\n",
    "$ 1nn(s_1 \\cup s_2) $ is also <span style=\"color:blue\">blue</span> circle <br>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-b\n",
    "\n",
    "![3nn.png](images/3nn.png)\n",
    "\n",
    "<center>\n",
    "$ 3nn(s_1) $ is <span style=\"color:red\">red</span> circle, <br>\n",
    "$ 3nn(s_2) $ is <span style=\"color:blue\">blue</span> circle, <br>\n",
    "$ 3nn(s_1 \\cup s_2) $ is <span style=\"color:green\">green</span> circle <br>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-a\n",
    "\n",
    "![graph2a](images/graph_2a.png)\n",
    " - _For training error to be minimum we need to give_ ____k=1____ _That way the model will learn (well, memorize) the data perfectly and the training error will be_ ___0___\n",
    " - _But training error is not really an estimation for the test set. For example with k being 1, training error will be 0. Lets assume we give a test data at point x. The data is is not within out predicted model even though we calculated our training error to be 0. So we can see that training error may not be same for the test set especially for this k._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-b\n",
    "\n",
    "![graph2b](images/graph_2b.png)\n",
    "\n",
    " - _Consider the graph above_\n",
    " - _Since our data is in two seperate group made out of ___n___ points each no matter what point we chose as \"left out\" the closest neighbors will be on the same group so long as_ $ k < n-1 $. _The moment k goes over n-1 we have to include the other group and that will increase the error rate since the current data point is in seperate group._\n",
    " - _Since suggested k value for cross validation is_ $ (number of points - 1) $ _and we have n-1 points in our group we can conclude that the optimal value for k is:_\n",
    " \n",
    " > $ k = (n-1)-1 $ <br>\n",
    " > _for n = 7 from graph_ <br>\n",
    " > $ k = (7-1)-1 = 5 $\n",
    " \n",
    " - But I have no idea what the cross validation error would be.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-c\n",
    " - Using k that too high and too low has negative effects similar to those caused by _overfitting_ and _underfitting_\n",
    " - If the k is too large we reduce the variance of the data. That causes effects similar to _underfitting_ and overgeneralizes the data.\n",
    " - If the k is too small model literally memorizes the training data. That causes _overfitting_ issue and since there is no generalization at all the data we will test with model will more likely to give increased error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-d\n",
    "![graph2d](images/graph_2d.png)\n",
    "<center>\n",
    "    <i>powered by paint</i>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-a (Linear Algebra)\n",
    "\n",
    "Coming soon to jupyter notebooks near your location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What?\n",
    "In this part I implemented a book recommendation system with KNN algorithm that does not recommend any book whatsoever. I used 3 different similarity functions and 2 different data parsing approach in this program. \n",
    "\n",
    "### How?\n",
    "Idea is to parse given sample data, calculate similarities with user-based approach then make a prediction based on those.\n",
    "Training and test data are parsed into either objects or matrices depending on the input.\n",
    "These data then send into similarity function of choice to get K nearest neighbors.\n",
    "Then using the neighbors a algorithm makes a prediction for books that given test users are already rated (for some reason) and using those predictions algorithm lastly calculates a score for accuracy (MAE)\n",
    "\n",
    "### More on How\n",
    "The predictions made with weighted and non-weighted algorithms. The weight distances are calculated similarities.\n",
    "***\n",
    " - _The prediction for the weighted is_: \n",
    "\n",
    "> $ p_{u,i} =  \\frac{\\sum_{N}{(S_{u,N} \\times R_{N,i})}}{\\sum_{N}{(|S_{u,N}|)}} $\n",
    "\n",
    "_For_ ___u___ _being the test user and_ ___i___ _being the current book_\n",
    "***\n",
    "\n",
    " - _The prediction  for the non weighted is_:\n",
    "\n",
    "> $ p_{u,i} =  \\frac{\\sum_{N}{(R_{N,i})}}{K} $\n",
    "\n",
    "_For_ ___k___ _being the nearest neighbor count._\n",
    "\n",
    "The weighted scores will be discussed on threshold part.\n",
    "***\n",
    "\n",
    "The similarity calculation can be done through the 3 algorithms:\n",
    " - Cosine similarity\n",
    " - Adjusted cosine similarity\n",
    " - Corelation based similarity\n",
    " \n",
    "I have tested all three algorithms and saw that ___Adjusted cosine similarity___ gave the best result.\n",
    "My test results for weighted prediction with 3000 unique test users were:\n",
    "\n",
    "|Algorithm|Score|\n",
    "|-|-|\n",
    "|Cosine similarity|3.87|\n",
    "|Corelation Based Similarity|1.5|\n",
    "|Adjusted Cosine Similarity|1.3|\n",
    "\n",
    "***\n",
    "\n",
    "I also have two different data parse option which needs two seperate KNN algorithms. These KNNs use same math but different data.\n",
    "Initally when I started the project I chose the obvious numpy way for KNN. But since the data is painfully sparse it took quite a bit of time to calculate everything it needed.\n",
    "So, since I did not want to use 3rd party library like _scipy_, I decided to add another method using _objects_ and _dictionaries_. This way the data will stay dense and the calculations took ___4-10___ times shorter. Also the matrix method only uses _cosine similarity_ so the accuracy is way, ___way___ low.\n",
    "\n",
    "|Method|Time(s)|Score|\n",
    "|-|-|-|\n",
    "|Matrix|~400|4.8|\n",
    "|Object|55|1.3|\n",
    "\n",
    "But I didn't remove the matrix method just yet. I believe in the case of a large/dense data (each user reading thousands of books so, another country :D ) the matrix method will show similar times as previous data while dictionary method might take much longer time like 100x.\n",
    "\n",
    "***\n",
    "\n",
    "There is also threshold issue, where i believe things go... wrong? <br>\n",
    "So I implemented the threshold on the rating prediction instead of, well, data parsing. The way it checks is if a book is rated less than a certain times it does not included in prediction calculation. <br>\n",
    "This actually improves results by ___tons___. <br>\n",
    "Only issue is this actually somehow improves the non-weighted score more than weighted one.\n",
    "\n",
    "|Threshold|Weighted score|Non-weighted score|\n",
    "|-|-|-|\n",
    "|0|1.3|1.5|\n",
    "|50|0.3|0.3|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
